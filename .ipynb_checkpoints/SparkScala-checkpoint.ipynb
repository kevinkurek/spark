{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2\n",
      "Map(0 -> 1, 1 -> 1, 6 -> 1, 9 -> 1, 2 -> 1, 3 -> 2)\n",
      "Map(b -> 1, a -> 1, c -> 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c: Seq[Int] = List(1, 2, 3, 3, 6, 9, 0)\n",
       "d: scala.collection.immutable.Vector[String] = Vector(a, b, c, c)\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c: Seq[Int] = Seq(1, 2, 3, 3, 6, 9, 0)\n",
    "println(c.size)\n",
    "val d = Vector(\"a\", \"b\", \"c\", \"c\")\n",
    "println(c.count(_==3))\n",
    "\n",
    "println(c.groupBy(identity).mapValues(_.size))\n",
    "println(d.groupBy(identity).mapValues(_.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma(a: Int) = a + 3\n",
    "suma(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x = 1 to 10\n",
    "val double = (i:Int) => i * 2\n",
    "val double_list = x.map(double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val final_sum = double_list.foldLeft(0.0)(_+_)\n",
    "println(final_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericRange a to z\n",
      "List(a, b, c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alphabet: scala.collection.immutable.NumericRange.Inclusive[Char] = NumericRange a to z\n",
       "alpha: List[Char] = List(a, b, c)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creates Numeric alphabet\n",
    "// val alphabet = 'a' to 'z'\n",
    "val alphabet = ('a' to 'z')\n",
    "val alpha = List('a','b','c')\n",
    "println(alphabet)\n",
    "println(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donuts: Seq[String] = List(Plain, Strawberry, Glazed)\n",
       "donut_words: String = \"Plain Donut Strawberry Donut Glazed Donut \"\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// def foldLeft[B](z: B)(op: (B, A) â‡’ B): B\n",
    "val donuts: Seq[String] = Seq(\"Plain\", \"Strawberry\", \"Glazed\")\n",
    "val donut_words = donuts.foldLeft(\"\")((a, b) => a + b + \" Donut \")\n",
    "\n",
    "// val donut_words = donuts.foldLeft(\"\")((a, b) => b + \" Donut \") // Glazed Donut (last entry)\n",
    "// println(s\"All donuts = ${donuts.foldLeft(\"\")((a, b) => a + b + \" Donut \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val r = alphabet.reverse.toList\n",
    "val smash = r.foldRight(\"\")(_+_)\n",
    "println(smash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chars: scala.collection.immutable.IndexedSeq[Char] = Vector(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// push collections together\n",
    "val chars = ('a' to 'z') ++ ('A' to 'Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML + Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.4:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1607981557397)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+-----+\n",
      "|advance| los|pax| paid|\n",
      "+-------+----+---+-----+\n",
      "|    1.0| 5.0|1.0|222.2|\n",
      "|    1.0| 0.0|2.0|442.2|\n",
      "|    4.0| 0.0|1.0|207.1|\n",
      "|    1.0| 0.0|1.0|249.1|\n",
      "|   12.0|18.0|2.0|628.4|\n",
      "+-------+----+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "StructType(StructField(advance,StringType,true), StructField(los,StringType,true), StructField(pax,StringType,true), StructField(paid,StringType,true))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@127d4660\n",
       "df: org.apache.spark.sql.DataFrame = [advance: string, los: string ... 2 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [advance: string, los: string ... 2 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [advance: string, los: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialize\n",
    "val spark = org.apache.spark.sql.SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Spark CSV Reader\")\n",
    "        .getOrCreate;\n",
    "\n",
    "// Import to Spark df\n",
    "var df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"df_pandas.csv\")\n",
    "df = df.drop(\"index\")\n",
    "df = df.select(\"advance\", \"los\", \"pax\", \"paid\")\n",
    "df.show(5)\n",
    "println(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(StructField(advance,FloatType,true), StructField(los,FloatType,true), StructField(pax,FloatType,true), StructField(paid,FloatType,true))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "castedDF: org.apache.spark.sql.DataFrame = [advance: float, los: float ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val donut_words = donuts.foldLeft(\"\") // starting value goes in ()\n",
    "                            //((\n",
    "                            //a, current values of list\n",
    "                            //b) newest item in list\n",
    "                            //=> a + b + \" Donut \") concat together current values in list with newest item in list\n",
    "\n",
    "// Convert all cols to float\n",
    "val castedDF = df.columns.foldLeft(df)((current, c) => current.withColumn(c, col(c).cast(\"float\")))\n",
    "println(castedDF.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assemble to Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns to vector column 'features'\n",
      "+-------+----+---+-----+--------------------+\n",
      "|advance| los|pax| paid|            features|\n",
      "+-------+----+---+-----+--------------------+\n",
      "|    1.0| 5.0|1.0|222.2|[1.0,5.0,1.0,222....|\n",
      "|    1.0| 0.0|2.0|442.2|[1.0,0.0,2.0,442....|\n",
      "|    4.0| 0.0|1.0|207.1|[4.0,0.0,1.0,207....|\n",
      "|    1.0| 0.0|1.0|249.1|[1.0,0.0,1.0,249....|\n",
      "|   12.0|18.0|2.0|628.4|[12.0,18.0,2.0,62...|\n",
      "+-------+----+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_c1639385a496, handleInvalid=error, numInputCols=4\n",
       "output: org.apache.spark.sql.DataFrame = [advance: float, los: float ... 3 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "// convert to dense vector feature column\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(castedDF.columns)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "// transform data\n",
    "val output = assembler.transform(castedDF)\n",
    "println(\"Assembled columns to vector column 'features'\")\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+-----+--------------------+--------------------+\n",
      "|advance| los|pax| paid|            features|      scaledFeatures|\n",
      "+-------+----+---+-----+--------------------+--------------------+\n",
      "|    1.0| 5.0|1.0|222.2|[1.0,5.0,1.0,222....|[0.02434390012342...|\n",
      "|    1.0| 0.0|2.0|442.2|[1.0,0.0,2.0,442....|[0.02434390012342...|\n",
      "|    4.0| 0.0|1.0|207.1|[4.0,0.0,1.0,207....|[0.09737560049369...|\n",
      "|    1.0| 0.0|1.0|249.1|[1.0,0.0,1.0,249....|[0.02434390012342...|\n",
      "|   12.0|18.0|2.0|628.4|[12.0,18.0,2.0,62...|[0.29212680148107...|\n",
      "+-------+----+---+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_96662d5317ef\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_96662d5317ef, numFeatures=4, withMean=false, withStd=true\n",
       "scaledData: org.apache.spark.sql.DataFrame = [advance: float, los: float ... 4 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "// Initialize Scaler\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "// Compute summary statistics by fitting the StandardScaler.\n",
    "val scalerModel = scaler.fit(output)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scalerModel.transform(output)\n",
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.02434390012342...|\n",
      "|[0.02434390012342...|\n",
      "|[0.09737560049369...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster_input: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Only grab scaled features then rename to features\n",
    "val cluster_input = scaledData.select(\"scaledFeatures\").withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "cluster_input.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.4957853443785829\n",
      "Cluster Centers: \n",
      "[0.9656586930619593,4.199334110006287,1.3732132708016558,0.8856660921141534]\n",
      "[0.5252672801325673,0.4071165697085678,1.0953494606871508,0.5011525193840705]\n",
      "[0.7451700564523532,0.569683853233855,2.4237409901042137,1.2378200980845149]\n",
      "[1.6357300487850643,0.9443531201628226,4.39560592487628,3.850356921350474]\n",
      "[2.9724173878954794,0.7162076257176955,1.6839187041990253,1.0679513058799524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_f70638c5f72f\n",
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_f70638c5f72f, k=5, distanceMeasure=euclidean, numFeatures=4\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: int]\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_95260352ae37, metricName=silhouette, distanceMeasure=squaredEuclidean\n",
       "silhouette: Double = 0.4957853443785829\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "\n",
    "// Trains a k-means model.\n",
    "val kmeans = new KMeans().setK(5).setSeed(1L)\n",
    "val model = kmeans.fit(cluster_input)\n",
    "\n",
    "// Make predictions\n",
    "val predictions = model.transform(cluster_input)\n",
    "\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.02434390012342...|         1|\n",
      "|[0.02434390012342...|         2|\n",
      "|[0.09737560049369...|         1|\n",
      "|[0.02434390012342...|         1|\n",
      "|[0.29212680148107...|         0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(StructField(advance,FloatType,true), StructField(los,FloatType,true), StructField(pax,FloatType,true), StructField(paid,FloatType,true))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@127d4660\n",
       "df: org.apache.spark.sql.DataFrame = [advance: string, los: string ... 2 more fields]\n",
       "castedDF: org.apache.spark.sql.DataFrame = [advance: float, los: float ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialize\n",
    "val spark = org.apache.spark.sql.SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Spark CSV Reader\")\n",
    "        .getOrCreate;\n",
    "\n",
    "// Import to Spark df\n",
    "val df = spark.read.format(\"csv\")\n",
    "                .option(\"header\",\"true\")\n",
    "                .load(\"df_pandas.csv\")\n",
    "                .drop(\"index\")\n",
    "                .select(\"advance\", \"los\", \"pax\", \"paid\")\n",
    "\n",
    "// Convert all cols to float\n",
    "val castedDF = df.columns.foldLeft(df)((current, c) => current.withColumn(c, col(c).cast(\"float\")))\n",
    "println(castedDF.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+-----+--------------------+--------------------+----------+\n",
      "|advance| los|pax| paid|   pre_scaled_vector|            features|prediction|\n",
      "+-------+----+---+-----+--------------------+--------------------+----------+\n",
      "|    1.0| 5.0|1.0|222.2|[1.0,5.0,1.0,222....|[0.02434390012342...|         1|\n",
      "|    1.0| 0.0|2.0|442.2|[1.0,0.0,2.0,442....|[0.02434390012342...|         2|\n",
      "|    4.0| 0.0|1.0|207.1|[4.0,0.0,1.0,207....|[0.09737560049369...|         1|\n",
      "|    1.0| 0.0|1.0|249.1|[1.0,0.0,1.0,249....|[0.02434390012342...|         1|\n",
      "|   12.0|18.0|2.0|628.4|[12.0,18.0,2.0,62...|[0.29212680148107...|         0|\n",
      "+-------+----+---+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_e13bf123c1b4, handleInvalid=error, numInputCols=4\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_de93d603d52b\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_8ce0783a5751\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_9cd507aa1559\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_9cd507aa1559\n",
       "pred: org.apache.spark.sql.DataFrame = [advance: float, los: float ... 5 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "\n",
    "// Configure an ML pipeline, which consists of 3 stages: VectorAssembler, StandardScaler, KMeans.\n",
    "// initialize vector assembler stage\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(castedDF.columns) // all columns in castedDF\n",
    "  .setOutputCol(\"pre_scaled_vector\")\n",
    "\n",
    "// initialize scaler stage\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(assembler.getOutputCol) // inputCol is output from assembler stage\n",
    "  .setOutputCol(\"features\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "// initialize kmeans stage\n",
    "val kmeans = new KMeans().setK(5).setSeed(1L)\n",
    "\n",
    "// initialize pipeline\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, scaler, kmeans))\n",
    "             \n",
    "// Fit pipeline\n",
    "val model = pipeline.fit(castedDF)\n",
    "             \n",
    "// Transform data\n",
    "val pred = model.transform(castedDF)\n",
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[org.apache.spark.ml.Transformer] = Array(VectorAssembler: uid=vecAssembler_e13bf123c1b4, handleInvalid=error, numInputCols=4, StandardScalerModel: uid=stdScal_de93d603d52b, numFeatures=4, withMean=false, withStd=true, KMeansModel: uid=kmeans_8ce0783a5751, k=5, distanceMeasure=euclidean, numFeatures=4)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show all stages of the pipeline\n",
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.4957853443785829\n",
      "Cluster Centers: \n",
      "[0.9656586930619593,4.199334110006287,1.3732132708016558,0.8856660921141534]\n",
      "[0.5252672801325673,0.4071165697085678,1.0953494606871508,0.5011525193840705]\n",
      "[0.7451700564523532,0.569683853233855,2.4237409901042137,1.2378200980845149]\n",
      "[1.6357300487850643,0.9443531201628226,4.39560592487628,3.850356921350474]\n",
      "[2.9724173878954794,0.7162076257176955,1.6839187041990253,1.0679513058799524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeansModel\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_09fb8bbfebe1, metricName=silhouette, distanceMeasure=squaredEuclidean\n",
       "silhouette: Double = 0.4957853443785829\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.{KMeansModel}\n",
    "\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(pred)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.stages.last.asInstanceOf[KMeansModel].clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Disk/Load from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9656586930619593,4.199334110006287,1.3732132708016558,0.8856660921141534]\n",
      "[0.5252672801325673,0.4071165697085678,1.0953494606871508,0.5011525193840705]\n",
      "[0.7451700564523532,0.569683853233855,2.4237409901042137,1.2378200980845149]\n",
      "[1.6357300487850643,0.9443531201628226,4.39560592487628,3.850356921350474]\n",
      "[2.9724173878954794,0.7162076257176955,1.6839187041990253,1.0679513058799524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sameModel: org.apache.spark.ml.PipelineModel = pipeline_9cd507aa1559\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"./tmp/spark-scala-KMeansPipeline\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-scala-KMeansPipeline\")\n",
    "\n",
    "sameModel.stages.last.asInstanceOf[KMeansModel].clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
